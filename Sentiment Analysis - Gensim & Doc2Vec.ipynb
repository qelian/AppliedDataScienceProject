{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "np.random.seed(0)\n",
    "import io\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn import cross_validation\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import pickle\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   stars                                               text\n",
      "0      1  Super simple place but amazing nonetheless. It...\n",
      "1      1  Small unassuming place that changes their menu...\n",
      "2      1  Lester's is located in a beautiful neighborhoo...\n",
      "3      1  Love coming here. Yes the place always needs t...\n",
      "4      1  Had their chocolate almond croissant and it wa...\n",
      "(296227, 2)\n"
     ]
    }
   ],
   "source": [
    "## Importing the dataset from an S3 storage location\n",
    "# create the s3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# this is the location of the data on S3 (usual)\n",
    "bucket='yelpreviewsdata' # put your S3 bucket name here\n",
    "prefix = 'data'\n",
    "obj = s3.get_object(Bucket=bucket, Key=f'{prefix}/yelp_review-0000.csv')\n",
    "df = pd.read_csv(io.BytesIO(obj['Body'].read()), encoding=\"iso-8859-15\", low_memory=True)\n",
    "# creating a dataframe from the attributes needed for the classifier\n",
    "df = df[['stars', 'text']]\n",
    "# binarizing the sentiment to 0s and 1s\n",
    "df['stars']= np.where(df['stars'] >=2, 1,0)\n",
    "print(df.head())\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling from the dataset due to limitations preventing us from vectorizing the full set\n",
    "def sampling_dataset(df):\n",
    "    count = 150000\n",
    "    class_df_sampled = pd.DataFrame(columns = [\"stars\",\"text\"])\n",
    "    temp = []\n",
    "    for c in df.stars.unique():\n",
    "        class_indexes = df[df.stars == c].index\n",
    "        random_indexes = np.random.choice(class_indexes, count, replace=True)\n",
    "        temp.append(df.loc[random_indexes])\n",
    "        \n",
    "    for each_df in temp:\n",
    "        class_df_sampled = pd.concat([class_df_sampled,each_df],axis=0)\n",
    "    \n",
    "    return class_df_sampled\n",
    "\n",
    "df = sampling_dataset(df)\n",
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py:566: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
      "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py:570: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "#labelling the rating text to be fed into the model\n",
    "lmtzr = WordNetLemmatizer()\n",
    "w = re.compile(\"\\w+\",re.I)\n",
    "\n",
    "def label_sentences(df):\n",
    "    labeled_sentences = []\n",
    "    for index, datapoint in df.iterrows():\n",
    "        tokenized_words = re.findall(w,datapoint[\"text\"].lower())\n",
    "        labeled_sentences.append(LabeledSentence(words=tokenized_words, tags=['SENT_%s' %index]))\n",
    "    return labeled_sentences\n",
    "\n",
    "def train_doc2vec_model(labeled_sentences):\n",
    "    model = gensim.models.Doc2Vec(size=300, window=10, min_count=5, workers=11,alpha=0.025, min_alpha=0.025, iter=20)\n",
    "    model.build_vocab(labeled_sentences)\n",
    "    model.train(labeled_sentences, epochs=model.iter, total_examples=model.corpus_count)\n",
    "    return model\n",
    "\n",
    "sen = label_sentences(df)\n",
    "# training the doc2vec model\n",
    "model = train_doc2vec_model(sen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  stars                                               text  \\\n",
      "0     1  The VIP has one of the best chicken wings in t...   \n",
      "1     1  We so wanted this new restaurant and brew hous...   \n",
      "2     1  I wasn't sure if I'd like Pilates on the refor...   \n",
      "3     1  This place is awesome. Lots of great rolls at ...   \n",
      "4     1  Super delicious Hainanese chicken and rice joi...   \n",
      "\n",
      "                                  vectorized_ratings  \n",
      "0  [0.7370991, 0.1282141, 0.88223875, 0.7988065, ...  \n",
      "1  [0.4672933, -0.61399937, 0.10503226, 0.8451672...  \n",
      "2  [0.72146267, 0.8445171, -1.6559016, 1.0426472,...  \n",
      "3  [-0.16004893, 0.5494206, 0.1510869, 1.3756956,...  \n",
      "4  [-0.43464002, 0.74114853, -0.5584688, 0.765369...  \n"
     ]
    }
   ],
   "source": [
    "#pre-defined function to vectorize the text provided for each rating using the doc2vec model trained earlier\n",
    "def vectorize_ratings(df,d2v_model):\n",
    "    y = []\n",
    "    ratings = []\n",
    "    for i in range(0,df.shape[0]):\n",
    "        label = 'SENT_%s' %i\n",
    "        ratings.append(d2v_model.docvecs[label])\n",
    "    df['vectorized_ratings'] = ratings\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = vectorize_ratings(df,model)\n",
    "print (df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting for cross-validation\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(df[\"vectorized_ratings\"].T.tolist(), df[\"stars\"], test_size=0.02, random_state=17)\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Classifier score 0.7076666666666667\n"
     ]
    }
   ],
   "source": [
    "# logistic regression classifier\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "preds = logreg.predict(X_test)\n",
    "print(\"Logistic Classifier score\", sum(preds == y_test) / len(y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(X,y):\n",
    "    n_estimators = [200,400]\n",
    "    min_samples_split = [2]\n",
    "    min_samples_leaf = [1]\n",
    "    bootstrap = [True]\n",
    "\n",
    "    parameters = {'n_estimators': n_estimators, 'min_samples_leaf': min_samples_leaf,\n",
    "                  'min_samples_split': min_samples_split}\n",
    "\n",
    "    clf = GridSearchCV(RFC(verbose=1,n_jobs=4), cv=4, param_grid=parameters)\n",
    "    clf.fit(X, y)\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV examined 30 candidate parameter settings.\n",
      "Model with rank: 1\n",
      "Mean OOB score: 0.583 (std: 0.001)\n",
      "Parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 3, 'min_samples_leaf': 1}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean OOB score: 0.579 (std: 0.001)\n",
      "Parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 2, 'min_samples_leaf': 1}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean OOB score: 0.579 (std: 0.001)\n",
      "Parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 2, 'min_samples_leaf': 1}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean OOB score: 0.578 (std: 0.001)\n",
      "Parameters: {'criterion': 'gini', 'max_depth': None, 'max_features': 3, 'min_samples_leaf': 1}\n",
      "\n",
      "Model with rank: 5\n",
      "Mean OOB score: 0.578 (std: 0.002)\n",
      "Parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 2, 'min_samples_leaf': 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Randomized search for model selection\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=5):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean OOB score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 4),\n",
    "              \"min_samples_leaf\": sp_randint(1, 5),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 30\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "#this might take a minute to run\n",
    "print(\"RandomizedSearchCV examined %d candidate parameter settings.\" % (n_iter_search))\n",
    "report(random_search.cv_results_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "confusion matrix\n",
      "Predicted     0     1   All\n",
      "True                       \n",
      "0          1847  1167  3014\n",
      "1          1178  1808  2986\n",
      "All        3025  2975  6000\n",
      "\n",
      "auc score 0.6091495992579394\n"
     ]
    }
   ],
   "source": [
    "# Running the model with the best score from the CV above\n",
    "clf = tree.DecisionTreeClassifier(max_features = 3, criterion = 'entropy', min_samples_leaf = 1)\n",
    "\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "y_pred=clf.predict(X_test)\n",
    "y_scores=clf.predict_proba(X_test)\n",
    "print ('\\nconfusion matrix')\n",
    "print(pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "print('\\nauc score '+str(auc(false_positive_rate, true_positive_rate)))\n",
    "\n",
    "#show a tradeoff curve for precision vs recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian NB score 0.5608333333333333\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "preds = gnb.predict(X_test)\n",
    "print(\"Gaussian NB score\", sum(preds == y_test) / len(y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB score 0.6348333333333334\n"
     ]
    }
   ],
   "source": [
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train, y_train)\n",
    "preds = bnb.predict(X_test)\n",
    "print(\"Bernoulli NB score\", sum(preds == y_test) / len(y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#References:\n",
    "# Alexander Andrews, Content Based Text Classification with Doc2Vec and TensorFlow, https://blog.francium.tech/content-based-text-classification-with-doc2vec-and-tensorflow-efd1dd4f02a8\n",
    "#Tushar Joshi , Sentiment Classification in Doc2Vec https://www.kaggle.com/tj2552/sentiment-classification-in-5-classes-doc2vec\n",
    "# David Batista, Document Classification, http://www.davidsbatista.net/blog/2017/04/01/document_classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
